
# the path to LDT working directory for cache and resources files.
path_to_resources: /the/path/to/cache/and/resources

# default query language for all dictionaries
default_language: English

# if you are working with embeddings that were made from a lowercased corpus,
# set to True.
lowercasing: True

# BabelNet key is required for querying BabelNet. Register at Babelnet.org/register
# for up to 1000 free queries a day.
# If you don't plan to use BabelNet, the parameter should be set to "None".
babelnet_key: None

# if True, a recent list of wiktionary entries will be downloaded to avoid
# queries for non-existing entries. The English file is about 60 Mb, so
# initial download takes a couple of seconds.
wiktionary_cache: True

# many LDT operations are expensive to compute. This option sets a limit on
# the size of cache that the library will use to store potentially
# re-useable information such as web queries. This is useful in large-scale
# experiments with multiple word embeddings, as many neighbor pairs may be the
# same. The value of this parameter has to be an integer or None.
cache_size: None

###############################################################################
# EXTRA RESOURCES:

# LDT expects to find the resources listed under the language_resources
# and corpus_resources in the specified path_to_resources. You can download
# the whole ldt_data folder here:
# https://my.pcloud.com/publink/show?code=XZnkSn7ZUdfgWrtnfiyuXhjg9I0es0iPQRWy
# Unpack it to the location of your choosing and specify that location in
# the config file as path_to_resources.

# Extra language-specific resources.

language_resources:
  en:
    names: names.vocab
    numbers: numbers.vocab
    associations: associations.json
    gdeps: gdeps.jsonl

# If corpus data is available, specify the folder name. Otherwise, set to None.
corpus: None
#corpus: Wiki201308
# the dump you can use to produce embeddings comparable
# to those in our experiments: http://ldtoolkit.space/task_data/

corpus_resources:
# the pre-computed resources for our corpus
  Wiki201308:
    freqdict: Wiki201308.freqdict
    vocabulary: Wiki201308.vocab
    cooccurrence: 3grams.jsonl

###############################################################################

experiments:

  # the vocabulary dataset to be tested, expected to be a single-column list of words
  # with .vocab extension. # Provide the full path to the .vocab file here, e.g.

  # vocab_sample: /path/to/test_sample.vocab

  # Alternatively, if you want to make use of vecto-style metadata, provide the name of
  # the your_dataset subfolder of vocab_samples subfolder in the general ldt resource location.
  # with one metadata.json file per subfolder.

  vocab_sample: test_sample

  #embeddings will be loaded with vecto library from the specified directory:
  embeddings: [
  /path/to/embeddings/dir,
  /path/to/embeddings/dir2
  ]

  # default default_workflow settings:

  # The data will be saved in "experiments" subfolder of the above path_to_resources location.
  # LDT analysis is performed in three steps: neighbor extraction, neighbor annotation, and
  # analysis of results. A subfolder will be created for each of these steps.

  # experiment name (will also be used as a subfolder name for the experiment data)
  experiment_name: testing

  # how many neighbors to extract and annotate
  top_n: 10

  # overwriting any previous data, if an experiment with the same name has already been performed:
  overwrite: True

  # how many CPUs to use for the annotation jobs.
  multiprocessing: 1

  #number of word pairs in a multiprocessing batch: more is more efficient, but at a higher risk of data loss
  batch_size: 1000

  # timeout for individual word pair queries: some words take longer to analyze
  # and drag down the whole process. Timeout of 30 seconds loses about 10
  # words in a thousand in ld_909 vocab sample on Wikipedia embeddings.
  timeout: 30